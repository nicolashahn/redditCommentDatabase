# redditCommentDatabase

For use on dataset you can find here:
https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/

Raw JSON -> MySQL database using Python + sqlalchemy + snudown libraries
- You can get sqlalchemy from pip (pip install sqlalchemy)
- snudown you need to download from github and install using setup.py (python setup.py install)

JSONtoMySQL.py:
- Organizes comments according to the Internet Arguments Corpus schema, storing table information for
  + Subreddit (seperate from Topic)
  + Discussion (link)
  + Author (username)
  + Basic Markup
  + Text
  + Post
- Strips Reddit's markdown from the text and creates a basic_markdown row for each instance of markdown. The goal is to eventually discover features indicating sarcasm. Given Reddit's r/sarcasm Subreddit, there should be a good amount of data to be gleaned from the 53 million comments in the 1 month dataset, or 1.7 billion in the entire Reddit comment dataset.

splitRedditDataset.py:
- takes any large file and splits it into 500,000 line chunks

splitFilesToMySQL.py:
- takes the series of files generated by splitRedditDataset.py and batch processes them using JSONtoMySQL.py


The schema varies slightly from the basic IAC database. The SQL script redditStructure will do the following:

Add row to datasets for reddit (default dataset_id = 6, change reddit_id in script)

Add a 'subreddits' table
  - dataset_id: tinyInt(3) PK
  - subreddit_id: int(20) UN PK
  - subreddit_name: varchar(255)
  - subreddit_native_id(20)

Add to 'basic_markup' table: markup_group_id: int(11)
  - useful for grouping nested quotes, lists
    
Change in 'discussions' AND 'posts' tables: native_discussion_id from int(10) -> varchar(12)
  - because reddit stores native ids as strings

